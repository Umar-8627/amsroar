\section{
    \textcolor{white}{
        Defining remote queues on AMSjobs interface.
    }
    \label{sec3:defq}
}
Submitting AMS remote jobs requires defining the appropriate queues that gives the program information about the remote host and requested resources. 
Roar uses a PBS scheduler the defined queue must be of the same type. 
A new queue can be created manually from the AMSjobs interface. 
Navigate to \codeinline{Queue -> New -> PBS}. 
Detailed information about each field on the queue definition window is available at the following \href{https://www.scm.com/doc/GUI/AMSjobs.html?highlight=pbs#setting-up-your-own-queues}{link}. 
Shown below in fig. \ref{fig:min_queue} are the settings required to submit remote jobs to Roar. 
The remote user field needs to be set appropriately.  
\begin{figure}[H]
    \includegraphics[width=\textwidth]{images/min_queue.PNG}
    \caption{Test queue definition on AMSjobs.
        \label{fig:min_queue}
    }
\end{figure}
Here we provide explanations for some of the more complex fields in the above queue definition. 
\begin{itemize}
    \item 
    \codeinline{Default Options:} field can be used as an additional bash variable for run command. 
    The label of this field is slightly misleading, since only one variable can be set here despite the trailing `s' in \codeinline{Default Options:} seeming to imply otherwise. 
    In our queue in fig. \ref{fig:min_queue} we use it for the default walltime requested. 
    The advantage of this field is that this value can be altered from job to job without editing the queue definition. 
    \item 
    We set the \codeinline{Remote job directory:} field to \codeinline{$HOME/scratch/AMSjobs}. 
    On Roar we recommend this setting since the computational chemistry jobs performed could produce files that require considerable storage space. 
    Using a directory in the scratch folder here will avoid jobs being interrupted due to files exceeding storage quotas provided to users.
    \item 
    \codeinline{Run command:}, here we provide the \codeinline{qsub} command to schedule the job on the remote machine. 
    Listed below are the various flags and options. 
    Additional optional arguments may be used if necessary.
    \input{tables/sec3_flag_table.tex}
    \item 
    \codeinline{Prolog command:}, commands included here are effectively prepended to the job script generated by AMS. 
    We use this field to load the AMS module on Roar. 
    Additionally, the command \codeinline{cd $PBS_O_WORKDIR} changes the compute nodes directory to the job directory. 
    Flag options such as: queue alloction, nodes, processors per node, memory per processor can vary from job to job. 
    To avoid editing the queue definition every time we recommend defining multiple queues with various possible combinations for these flag options.
    \item 
    \codeinline{Queue name:} here we used a place holder name for the queue. 
    However, we recommend using names indicative of the variable flag options. 
    For example, `open\_1\_4\_4' - indicating this queue is using the open allocation and requesting 1 node, 4 processors per node, and 4~GB memory per node - could be a suitable name for the queue defined in fig. \ref{fig:min_queue}.
\end{itemize}
The queue definition discussed above is saved by AMS as \codeinline{<Queue name>.tid} file in the \codeinline{$HOME/.scm_gui/} folder. 
The file is stored as follows.
\lstinputlisting[
    basicstyle=\ttfamily\scriptsize,
    language=bash
]{images/roar-test.tid}
For readers convenience we have created a bash script to automate defining new queues.
The script produces \codeinline{<Queue name>.tid} files such as the one shown above. 
The script accepts two mandatory arguments providing the number of nodes and processors per node to be requested by the queue. 
Additionally optional arguments may be passed to the script to alter the options listed in the table below. 
\input{tables/sec3_opt_arg.tex}
